{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "key_word_model1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreasfloros/ARM-ML-Embedded/blob/main/audio_classifier_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDKVJZ0SnY8z"
      },
      "source": [
        "# Audio classifier pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhjmJcqIe4Ku"
      },
      "source": [
        "### 1) Download and untar dataset directly to colab:\n",
        "\n",
        "To change between datasets change the url to one which downloads a .tar file.\n",
        "\n",
        "**Other important notes:**\n",
        "\n",
        "**1.** If the audio_data folder already exists in your collab session running the following cell will give an error. That is not a problem, run it anyway.\n",
        "\n",
        "**2.** To load the audio samples we use the librosa library. Its load function supports a large range of input formats such as WAV, MP3, FLAC, OGG and many others. For the full list of available input formats please refer to the documentation at https://librosa.org/doc/main/generated/librosa.load.html. An important point is that the library can't load README files, which can often be found in datasets as .md and .txt files. Not deleting such files will cause the library  to fail when processing the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hROJGZ_rWPGa"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "DATASET_ROOT_DIR = 'audio_data/'\n",
        "os.mkdir(DATASET_ROOT_DIR)\n",
        "url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "target_path = 'audio_data/dataset.tar.gz'\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(target_path, 'wb') as f:\n",
        "        f.write(response.raw.read())\n",
        "\n",
        "tar = tarfile.open(target_path, \"r:gz\")\n",
        "tar.extractall(path='audio_data/')\n",
        "tar.close()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_mTATcwlAle"
      },
      "source": [
        "### 2) Preprocess dataset:\n",
        "\n",
        "We iterate through the entire dataset, process every audio track using a processing method (either FFT, STFT, or MFCC), and store all of them in a JSON file. Currently, the user is able to control the processing method, expected duration of the audio tracks, the sample rate, as well as other values related to the processing methods.\n",
        "\n",
        "Potentially, the user might also be able (in the future) to automatically go through all processing methods and use the one which optimizes the current model accuracy.\n",
        "\n",
        "The process explained above can be completed through the following steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hzEODswh24"
      },
      "source": [
        "**Step 1:** Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q868TWmywSTW"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import math\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jsjd9sGwsbw"
      },
      "source": [
        "**Step 2:** Function to extend/cut tracks appropriately so that all contain the expected number of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExNJOdGAwUS3"
      },
      "source": [
        "def make_track_correct_size(signal, expected_num_samples_per_track):\n",
        "\n",
        "    # print('Original track length: {}'.format(len(signal)))\n",
        "    # if track is shorter than expected, append it with zeros\n",
        "    if len(signal) < expected_num_samples_per_track:\n",
        "      num_zeros_to_pad = expected_num_samples_per_track - len(signal)\n",
        "      zeros = num_zeros_to_pad * [0.]\n",
        "      extended_signal = np.append(signal, zeros)\n",
        "      return extended_signal\n",
        "\n",
        "    # if track is longer than expected, truncate it\n",
        "    elif len(signal) > expected_num_samples_per_track:\n",
        "      return signal[:expected_num_samples_per_track]\n",
        "\n",
        "    # else return the original track \n",
        "    else:\n",
        "      return signal"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5KBOt9mw9o-"
      },
      "source": [
        "**Step 3:** Define function to process a single track using the specified method (FFT/STFT/MFCC) and return the data structure containing the result. This function will be called for all tracks within the dataset. Also note that we always return a 1D array by flattening the 2D arrays where necessary. We do this so that we can replicate the exact same preprocessing on Arduino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ndxpmhtwYhw"
      },
      "source": [
        "def audio_track_to_features(signal, processing_method, sample_rate, window_size, window_stride, num_mfcc):\n",
        "  \n",
        "  if processing_method == 'fft':\n",
        "    # perform Fast Fourier Transform (FFT)\n",
        "    fft = np.fft.fft(signal)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrum = np.abs(fft)\n",
        "\n",
        "    # the spectrum is symmetrical with respect to sample_rate / 2\n",
        "    # so take half of the spectrum and frequency arrays\n",
        "    # therefore len(half_spectrum) = sample_rate / 2\n",
        "    half_spectrum = spectrum[:int(len(spectrum)/2)]\n",
        "\n",
        "    # average every 16 samples to reduce size of array to 1 / 16 of its original size\n",
        "    # e.g. sample_rate = 16k, duration = 1.024s, reduce size from 8192 to 512 \n",
        "    averaged = np.mean(half_spectrum.reshape(-1, 16), axis=1)\n",
        "\n",
        "    # transform to range -128 to 127\n",
        "    averaged *= (255.0/averaged.max()) # 0-255\n",
        "    averaged -= 128\n",
        "\n",
        "    return averaged\n",
        "\n",
        "\n",
        "  elif processing_method == 'stft':\n",
        "    # perform Short Time Fourier Transform (STFT)\n",
        "    stft = librosa.stft(signal = signal, \n",
        "                        n_fft = window_size, \n",
        "                        hop_length = window_stride)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrogram = np.abs(stft)\n",
        "\n",
        "    # transpose and return the spectrogram matrix\n",
        "    transposed_spectrogram = spectrogram.transpose()\n",
        "    return transposed_spectrogram.flatten()\n",
        "\n",
        "\n",
        "  else: # mfcc\n",
        "    # perform Mel-Frequency Cepstral Coefficients (MFCC)\n",
        "    mfcc = librosa.feature.mfcc(signal, \n",
        "                                sr = sample_rate, \n",
        "                                n_fft = window_size, \n",
        "                                n_mfcc = num_mfcc,\n",
        "                                hop_length = window_stride)\n",
        "    # transpose and return the mfcc matrix\n",
        "    transposed_mfcc = mfcc.T\n",
        "    return transposed_mfcc.flatten()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcI71k9s1tg2"
      },
      "source": [
        "**Step 4:** Define function to process every audio track and create a JSON file with the entire processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBwlSAsbfyKi"
      },
      "source": [
        "def preprocess_entire_dataset(dataset_path, json_path, processing_method, sample_rate, expected_duration, window_size, window_stride, num_mfcc):\n",
        "  # expected duration is in seconds\n",
        "  expected_num_samples_per_track = int(expected_duration * sample_rate)\n",
        "  \n",
        "  # dictionary to later be converted to final json file\n",
        "  data = {\n",
        "      'mapping' : [],\n",
        "      'features' : [],\n",
        "      'labels' : []\n",
        "  }\n",
        "\n",
        "  # we will iterate this for each of the visited sub-directorie in order to\n",
        "  # give a different label for each of them\n",
        "  visited_directory_index = 0\n",
        "\n",
        "  # iterate through all subfolders\n",
        "  for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
        "\n",
        "    # # ensure we are not at the dataset root directory\n",
        "    # # (os.walk provides this directory as well)\n",
        "    # if dirpath is not DATASET_ROOT_DIR:\n",
        "    if dirpath == 'audio_data/yes' or dirpath == 'audio_data/no':\n",
        "\n",
        "      # obtain word labels\n",
        "      dirpath_components = dirpath.split('/') # audio_data/left => ['audio_data', 'left']\n",
        "      word_label = dirpath_components[-1]\n",
        "      data['mapping'].append(word_label)\n",
        "      print('Processing {}'.format(word_label))\n",
        "\n",
        "      # access and process files for current word\n",
        "      for f in filenames:\n",
        "\n",
        "        # load audio file\n",
        "        file_path = os.path.join(dirpath, f)\n",
        "        signal, sample_rate = librosa.load(file_path, sr=sample_rate)\n",
        "\n",
        "        # extend or cut signal to be equal to the expected size\n",
        "        signal_correct_size = make_track_correct_size(signal, expected_num_samples_per_track)\n",
        "\n",
        "        # obtain the features of the audio track using the function defined above\n",
        "        track_features = audio_track_to_features(signal = signal_correct_size, \n",
        "                                                processing_method = processing_method,\n",
        "                                                sample_rate = sample_rate, \n",
        "                                                window_size = window_size, \n",
        "                                                window_stride = window_stride, \n",
        "                                                num_mfcc = num_mfcc)\n",
        "        \n",
        "        # append the audio track features to the features field of the dictionary\n",
        "        data['features'].append(track_features.tolist())\n",
        "\n",
        "        # append the current directory index as the label of this track\n",
        "        data['labels'].append(visited_directory_index)\n",
        "        # print('file_path: {}'.format(file_path))\n",
        "\n",
        "      # iterate the index before visiting the next directory\n",
        "      visited_directory_index = visited_directory_index + 1\n",
        "\n",
        "  print(data['mapping'])\n",
        "  print(set(data['labels']))\n",
        "  # create the json file from the dictionary\n",
        "  with open(json_path, 'w') as fp:\n",
        "    json.dump(data, fp, indent=4)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kApAU17Yk2rS"
      },
      "source": [
        "**Step 5:** Before running these functions with the code in the following cell we must delete the '.ipynb_checkpoint' files which might otherwise be considered as part of the dataset and interfere with training the model. We also delete the README.md file in the audio_data/background_noise directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laA_vd7A02hn"
      },
      "source": [
        "rm -rf `find -type d -name .ipynb_checkpoints`"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJaCkd1Ny2BD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535f633e-7fb9-46ca-b04d-c6aad3fc1b82"
      },
      "source": [
        "!rm audio_data/_background_noise_/README.md"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'audio_data/_background_noise_/README.md': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNV13MzS1-2M"
      },
      "source": [
        "**Step 6:** Run the preprocessing function above with the desired parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQKXmoGxjKJx"
      },
      "source": [
        "JSON_PATH = 'data.json'\n",
        "PROCESSING_METHOD = 'fft'\n",
        "SAMPLE_RATE = 16000\n",
        "EXPECTED_DURATION = 1.024            # in seconds\n",
        "WINDOW_SIZE_SAMPLES = 512            # in samples\n",
        "WINDOW_STRIDE_SAMPLES = 320          # in samples\n",
        "MFCC_COEFF_NUMBER = 13\n",
        "\n",
        "preprocess_entire_dataset(dataset_path = DATASET_ROOT_DIR, \n",
        "                   json_path = JSON_PATH, \n",
        "                   processing_method = PROCESSING_METHOD,\n",
        "                   sample_rate = SAMPLE_RATE, \n",
        "                   expected_duration = EXPECTED_DURATION, \n",
        "                   window_size = WINDOW_SIZE_SAMPLES, \n",
        "                   window_stride = WINDOW_STRIDE_SAMPLES, \n",
        "                   num_mfcc = MFCC_COEFF_NUMBER)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nio7zr1n-f17"
      },
      "source": [
        "### 3) Build and evaluate model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoJGGu3MraXK"
      },
      "source": [
        "**Step 1:** Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSHmrMl1rjFd"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Reshape, Activation, BatchNormalization, Conv2D, MaxPooling2D, Flatten\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-VELpQkrmk9"
      },
      "source": [
        "**Step 2:** Define functions to load and prepare the dataset for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYFesrHkrvVX"
      },
      "source": [
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    print('Loading dataset')\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    X = np.array(data['features'])\n",
        "    y = np.array(data['labels'])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
        "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
        "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
        "    :return X_train (ndarray): Input training set\n",
        "    :return X_validation (ndarray): Input validation set\n",
        "    :return X_test (ndarray): Input test set\n",
        "    :return y_train (ndarray): Target training set\n",
        "    :return y_validation (ndarray): Target validation set\n",
        "    :return y_test (ndarray): Target test set\n",
        "    \"\"\"\n",
        "\n",
        "    print('Splitting dataset into training, validation, and test splits')\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(JSON_PATH)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # add an axis to input sets for conv networks to fit conv2D shape specs\n",
        "    # currently not needed since we reshape everything anyway\n",
        "    # X_train = X_train[..., np.newaxis]\n",
        "    # X_validation = X_validation[..., np.newaxis]\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzYXN1Qr5UQ"
      },
      "source": [
        "**Step 3:** Define seperate functions for building a dense and a convolutional model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpkZZWWWyOPR"
      },
      "source": [
        "def build_conv_model(input_shape, reshape_shape):\n",
        "    \"\"\"Generates CNN model\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    # build network topology\n",
        "    model = Sequential()\n",
        "\n",
        "    # reshape input from 1D to 2D\n",
        "    model.add(Reshape(reshape_shape, input_shape=input_shape))\n",
        "\n",
        "    # 1st conv layer\n",
        "    model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 2nd conv layer\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # flatten output and feed it into dense layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4dmf8Our4xf"
      },
      "source": [
        "def build_dense_model(input_shape):\n",
        "    \"\"\"Generates CNN model\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    # build network topology\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(16, activation='relu', input_shape=input_shape))\n",
        "    # model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    # model.add(Dense(256, activation='relu'))\n",
        "    # # model.add(Dropout(0.3))\n",
        "\n",
        "    # model.add(Dense(512, activation='relu'))\n",
        "    # # model.add(Dropout(0.3))\n",
        "\n",
        "    # model.add(Dense(256, activation='relu'))\n",
        "\n",
        "    # model.add(Dense(128, activation='relu'))\n",
        "    # # model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "\n",
        "    # output layer\n",
        "    model.add(Dense(36, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zV2CG0sAvQ"
      },
      "source": [
        "**Step 4:** Define function to plot the training history of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz9DRkNBuhTF"
      },
      "source": [
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhitZN4t_8y"
      },
      "source": [
        "**Step 5:** Run all of the above functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKb7aMsg6cdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d0a284-a51e-49b6-db7e-0798fac4dc5e"
      },
      "source": [
        "# get train, validation, test splits\n",
        "X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "print('Finished preparing training, validation, and test data')\n",
        "print('X_train.shape: {}'.format(X_train.shape))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting dataset into training, validation, and test splits\n",
            "Loading dataset\n",
            "Finished preparing training, validation, and test data\n",
            "X_train.shape: (4790, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvHUkZjX41S9"
      },
      "source": [
        "**Step 6:** Since when preprocessing the entire dataset we flattened all 2D arrays to 1D arrays, we calculate the original 2D dimentions in order to Reshape in the first layer of our Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Koahh8P0JwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba0e20b-6c5a-437f-cb3c-399697f1e0be"
      },
      "source": [
        "NUM_OF_SAMPLES = SAMPLE_RATE * EXPECTED_DURATION\n",
        "NUM_OF_WINDOW_POSITIONS = math.ceil(NUM_OF_SAMPLES / WINDOW_STRIDE_SAMPLES)\n",
        "NUM_OF_STFT_FREQUENCIES = int(1 + WINDOW_SIZE_SAMPLES / 2)\n",
        "\n",
        "FFT_ARRAY_SIZE = int((NUM_OF_SAMPLES) / (2 * 16))  # 2 comes from half spectrum, 16 from averaging every 16 samples\n",
        "\n",
        "if PROCESSING_METHOD == 'fft':\n",
        "  RESHAPE_SHAPE = (1, FFT_ARRAY_SIZE, 1)\n",
        "elif PROCESSING_METHOD == 'stft':\n",
        "  RESHAPE_SHAPE = (NUM_OF_WINDOW_POSITIONS, NUM_OF_STFT_FREQUENCIES, 1)\n",
        "else: # mfcc\n",
        "  RESHAPE_SHAPE = (NUM_OF_WINDOW_POSITIONS, MFCC_COEFF_NUMBER, 1)\n",
        "\n",
        "print(RESHAPE_SHAPE)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 512, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4xgyIift_ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca234422-35c7-4c50-809f-152744b65e37"
      },
      "source": [
        "network_is_convolutional = True\n",
        "\n",
        "# for convolutional model\n",
        "if network_is_convolutional:\n",
        "  input_shape = (X_train.shape[1],)\n",
        "  print(\"input_shape = {}\".format(input_shape))\n",
        "  model = build_conv_model(input_shape, RESHAPE_SHAPE)\n",
        "# for dense model\n",
        "else:\n",
        "  input_shape = (X_train.shape[1],)\n",
        "  print(\"input_shape = {}\".format(input_shape))\n",
        "  model = build_dense_model(input_shape)\n",
        "\n",
        "\n",
        "# compile model\n",
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# train model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=20)\n",
        "\n",
        "# plot accuracy/error for training and validation\n",
        "plot_history(history)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_shape = (512,)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 1, 512, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1, 512, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1, 256, 8)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1, 256, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1, 256, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 128, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 128, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 128, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 64, 32)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 64, 32)         128       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 26,624\n",
            "Trainable params: 26,512\n",
            "Non-trainable params: 112\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "150/150 [==============================] - 5s 35ms/step - loss: 0.4800 - accuracy: 0.7658 - val_loss: 0.6556 - val_accuracy: 0.5434\n",
            "Epoch 2/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.2535 - accuracy: 0.9033 - val_loss: 0.4289 - val_accuracy: 0.7813\n",
            "Epoch 3/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.2092 - accuracy: 0.9136 - val_loss: 0.3363 - val_accuracy: 0.8364\n",
            "Epoch 4/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1789 - accuracy: 0.9313 - val_loss: 0.2259 - val_accuracy: 0.9007\n",
            "Epoch 5/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1721 - accuracy: 0.9378 - val_loss: 0.2416 - val_accuracy: 0.8932\n",
            "Epoch 6/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1519 - accuracy: 0.9443 - val_loss: 0.1703 - val_accuracy: 0.9341\n",
            "Epoch 7/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1532 - accuracy: 0.9470 - val_loss: 0.1594 - val_accuracy: 0.9382\n",
            "Epoch 8/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1489 - accuracy: 0.9466 - val_loss: 0.2406 - val_accuracy: 0.8990\n",
            "Epoch 9/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1373 - accuracy: 0.9491 - val_loss: 0.2280 - val_accuracy: 0.9073\n",
            "Epoch 10/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1368 - accuracy: 0.9511 - val_loss: 0.2232 - val_accuracy: 0.9065\n",
            "Epoch 11/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1322 - accuracy: 0.9526 - val_loss: 0.1903 - val_accuracy: 0.9265\n",
            "Epoch 12/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1255 - accuracy: 0.9553 - val_loss: 0.1691 - val_accuracy: 0.9357\n",
            "Epoch 13/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1242 - accuracy: 0.9582 - val_loss: 0.1898 - val_accuracy: 0.9274\n",
            "Epoch 14/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1160 - accuracy: 0.9572 - val_loss: 0.2238 - val_accuracy: 0.9132\n",
            "Epoch 15/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1130 - accuracy: 0.9589 - val_loss: 0.1648 - val_accuracy: 0.9357\n",
            "Epoch 16/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1104 - accuracy: 0.9614 - val_loss: 0.1520 - val_accuracy: 0.9457\n",
            "Epoch 17/20\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1052 - accuracy: 0.9643 - val_loss: 0.1623 - val_accuracy: 0.9457\n",
            "Epoch 18/20\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 0.1010 - accuracy: 0.9643 - val_loss: 0.1677 - val_accuracy: 0.9407\n",
            "Epoch 19/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0979 - accuracy: 0.9637 - val_loss: 0.2284 - val_accuracy: 0.9207\n",
            "Epoch 20/20\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0956 - accuracy: 0.9678 - val_loss: 0.1469 - val_accuracy: 0.9457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-naJSlOJt4TL"
      },
      "source": [
        "**Step 7:** Define function to predict a single sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtDhQL0jt49-"
      },
      "source": [
        "def predict(model, X, y):\n",
        "    \"\"\"Predict a single sample using the trained model\n",
        "    :param model: Trained classifier\n",
        "    :param X: Input data\n",
        "    :param y (int): Target\n",
        "    \"\"\"\n",
        "\n",
        "    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n",
        "    X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)\n",
        "\n",
        "    # perform prediction\n",
        "    prediction = model.predict(X)\n",
        "\n",
        "    # get index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)\n",
        "\n",
        "    print(\"Target: {}, Predicted label: {}\".format(y, predicted_index))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UzQyblHauQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90990c2b-6350-4b59-9e03-eb46a9659e61"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# pick a sample to predict from the test set\n",
        "X_to_predict = X_test[100]\n",
        "y_to_predict = y_test[100]\n",
        "\n",
        "# predict sample\n",
        "predict(model, X_to_predict, y_to_predict)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 - 1s - loss: 0.1371 - accuracy: 0.9434\n",
            "\n",
            "Test accuracy: 0.943415105342865\n",
            "Target: 0, Predicted label: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym2VPa_GqUUG"
      },
      "source": [
        "### 4) Save model and convert to desired formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG0mAr41po2x"
      },
      "source": [
        "Save keras model as an .h5 file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Ay09Hr1CzD"
      },
      "source": [
        "model_name = 'yesno_conv_fft_normalized'\n",
        "model.save(model_name + '.h5')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4nJs7vvSew_"
      },
      "source": [
        "For converting to tflite model there is a CHANCE that we require tensorflow 2.2 specifically. So check the version with the cell directly below and if the version isn't 2.2 (or 2.2.something) then run the cell below it. If it is please don't uninstall and reinstall the same version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WruWSyKwucqB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8a313a8-47f8-4a5c-a389-ae5af98b3421"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ58MdsySZvV",
        "outputId": "2f5a6029-78b3-4639-a9e1-da5c96db14ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install  tensorflow==2.2"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "Collecting tensorflow==2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1a/0d79814736cfecc825ab8094b39648cc9c46af7af1bae839928acb73b4dd/tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     || 516.2MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (2.10.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     || 460kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.12.4)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     || 3.0MB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow==2.2) (56.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.28.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM4WBSa7p1-I"
      },
      "source": [
        "Convert the model to a (currently not quantized due to commented out lines) tflite model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEQYCXLUbbSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f9f429-f4ed-4c6d-b939-6d5bb541333c"
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# def representative_dataset_generator():\n",
        "#   for value in X_test:\n",
        "#     yield [np.array(value, dtype = np.float32, ndmin=4)]\n",
        "\n",
        "# converter.representative_dataset = representative_dataset_generator\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(model_name + '.tflite', \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(model_name + '.tflite')\n",
        "print(\"Model is %d bytes\" % basic_model_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is 111236 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt2aD7aUoEuS"
      },
      "source": [
        "Convert model from tflite file to byte array for deployment on Arduino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1OgjXKln-b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4db2c48-be58-4dcb-958b-4849643b475e"
      },
      "source": [
        "!apt-get -qq install xxd\n",
        "!xxd -i yesno_conv_fft_normalized.tflite > yesno_conv_fft_normalized.cc"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xxd.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../xxd_2%3a8.0.1453-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking xxd (2:8.0.1453-1ubuntu1.4) ...\n",
            "Setting up xxd (2:8.0.1453-1ubuntu1.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}