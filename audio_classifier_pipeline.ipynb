{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "key_word_model1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreasfloros/ARM-ML-Embedded/blob/main/audio_classifier_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhjmJcqIe4Ku"
      },
      "source": [
        "# Audio classifier pipeline\n",
        "### 1) Download and untar dataset directly to colab:\n",
        "\n",
        "To change between datasets change the url to one which downloads a .tar file.\n",
        "\n",
        "**For the Speech Commands dataset you MUST delete the README.md file in the audio_data/_background_noise_ subfolder, since this cannot be read by the audio processing library and which will cause it to fail when processing the dataset !**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hROJGZ_rWPGa",
        "outputId": "033a97fc-f7c1-48b4-c8e0-7619b3f3d6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "DATASET_ROOT_DIR = 'audio_data/'\n",
        "os.mkdir(DATASET_ROOT_DIR)\n",
        "url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "target_path = 'audio_data/dataset.tar.gz'\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(target_path, 'wb') as f:\n",
        "        f.write(response.raw.read())\n",
        "\n",
        "tar = tarfile.open(target_path, \"r:gz\")\n",
        "tar.extractall(path='audio_data/')\n",
        "tar.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-956d562ef128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDATASET_ROOT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'audio_data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_ROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'audio_data/dataset.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'audio_data/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_mTATcwlAle"
      },
      "source": [
        "### 2) Preprocess dataset:\n",
        "\n",
        "We iterate through the entire dataset, process every audio track using a processing method (either FFT, STFT, or MFCC), and store all of them in a JSON file. Currently, the user is able to control the processing method, expected duration of the audio tracks, the sample rate, as well as other values related to the processing methods.\n",
        "\n",
        "Potentially, the user might also be able (in the future) to automatically go through all processing methods and use the one which optimizes the current model accuracy.\n",
        "\n",
        "The process explained above can be completed through the following steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hzEODswh24"
      },
      "source": [
        "**Step 1:** Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q868TWmywSTW"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import math\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jsjd9sGwsbw"
      },
      "source": [
        "**Step 2:** Function to extend/cut tracks appropriately so that all contain the expected number of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExNJOdGAwUS3"
      },
      "source": [
        "def make_track_correct_size(signal, expected_num_samples_per_track):\n",
        "\n",
        "    # print('Original track length: {}'.format(len(signal)))\n",
        "    # if track is shorter than expected, append it with zeros\n",
        "    if len(signal) < expected_num_samples_per_track:\n",
        "      num_zeros_to_pad = expected_num_samples_per_track - len(signal)\n",
        "      zeros = num_zeros_to_pad * [0.]\n",
        "      extended_signal = np.append(signal, zeros)\n",
        "      return extended_signal\n",
        "\n",
        "    # if track is longer than expected, truncate it\n",
        "    elif len(signal) > expected_num_samples_per_track:\n",
        "      return signal[:expected_num_samples_per_track]\n",
        "\n",
        "    # else return the original track \n",
        "    else:\n",
        "      return signal"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5KBOt9mw9o-"
      },
      "source": [
        "**Step 3:** Define function to process a single track using the method specified as input and return the data structure containing the result. This function will be called for all tracks within the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ndxpmhtwYhw"
      },
      "source": [
        "def audio_track_to_features(signal, processing_method, sample_rate, window_size, window_stride, num_mfcc):\n",
        "\n",
        "  if processing_method == 'fft':\n",
        "    # perform Fast Fourier Transform (FFT)\n",
        "    fft = np.fft.fft(signal)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrum = np.abs(fft)\n",
        "\n",
        "    # the spectrum is symmetrical with respect to sample_rate / 2\n",
        "    # so take half of the spectrum and frequency arrays\n",
        "    # therefore len(half_spectrum) = sample_rate / 2\n",
        "    half_spectrum = spectrum[:int(len(spectrum)/2)]\n",
        "\n",
        "    # average every 10 samples to reduce size of array to 1 / 10 of its original size\n",
        "    averaged = np.mean(half_spectrum.reshape(-1, 10), axis=1)\n",
        "\n",
        "    # convert to a 2D shape to fit the neural network's Conv2D layer input shape specs\n",
        "    two_dimentional = np.reshape(averaged, (1, len(averaged)))\n",
        "    return two_dimentional\n",
        "\n",
        "\n",
        "  elif processing_method == 'stft':\n",
        "    # perform Short Time Fourier Transform (STFT)\n",
        "    stft = librosa.stft(signal, n_fft=window_size, hop_length=window_stride)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrogram = np.abs(stft)\n",
        "\n",
        "    # transpose and return the spectrogram matrix\n",
        "    transposed_spectrogram = spectrogram.transpose()\n",
        "    return transposed_spectrogram\n",
        "\n",
        "\n",
        "  else: # mfcc\n",
        "    # perform Mel-Frequency Cepstral Coefficients (MFCC)\n",
        "    mfcc = librosa.feature.mfcc(signal, \n",
        "                                sr = sample_rate, \n",
        "                                n_fft = window_size, \n",
        "                                n_mfcc = num_mfcc,\n",
        "                                hop_length = window_stride)\n",
        "    # transpose and return the mfcc matrix\n",
        "    transposed_mfcc = mfcc.T\n",
        "    return transposed_mfcc"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcI71k9s1tg2"
      },
      "source": [
        "**Step 4:** Define function to process every audio track and create a JSON file with the entire processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBwlSAsbfyKi"
      },
      "source": [
        "def preprocess_entire_dataset(dataset_path, json_path, processing_method, sample_rate, expected_duration, window_size, window_stride, num_mfcc):\n",
        "  # expected duration is in seconds\n",
        "  expected_num_samples_per_track = expected_duration * sample_rate\n",
        "  \n",
        "  # dictionary to later be converted to final json file\n",
        "  data = {\n",
        "      'mapping' : [],\n",
        "      'features' : [],\n",
        "      'labels' : []\n",
        "  }\n",
        "\n",
        "  # iterate through all subfolders\n",
        "  for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "\n",
        "    # # ensure we are not at the dataset root directory\n",
        "    # # (os.walk provides this directory as well)\n",
        "    if dirpath is not DATASET_ROOT_DIR:\n",
        "\n",
        "      # obtain word labels\n",
        "      dirpath_components = dirpath.split('/') # audio_data/left => ['audio_data', 'left']\n",
        "      word_label = dirpath_components[-1]\n",
        "      data['mapping'].append(word_label)\n",
        "      print('Processing {}'.format(word_label))\n",
        "\n",
        "      # access and process files for current word\n",
        "      for f in filenames:\n",
        "        \n",
        "        # load audio file\n",
        "        file_path = os.path.join(dirpath, f)\n",
        "        signal, sample_rate = librosa.load(file_path, sr=sample_rate)\n",
        "\n",
        "        # extend or cut signal to be equal to the expected size\n",
        "        signal_correct_size = make_track_correct_size(signal, expected_num_samples_per_track)\n",
        "\n",
        "        # obtain the features of the audio track using the function defined above\n",
        "        track_features = audio_track_to_features(signal = signal_correct_size, \n",
        "                                                 processing_method = 'fft', \n",
        "                                                 sample_rate = sample_rate, \n",
        "                                                 window_size = window_size, \n",
        "                                                 window_stride = window_stride, \n",
        "                                                 num_mfcc = 13)\n",
        "        \n",
        "        # append the audio track features to the features field of the dictionary\n",
        "        data['features'].append(track_features.tolist())\n",
        "\n",
        "        # append the current index-1 as the label of this track\n",
        "        # the -1 comes from the fact that the index 0 refers to the dataset root directory,\n",
        "        # the only directory which does not have a label\n",
        "        data['labels'].append(i-1)\n",
        "        # print('file_path: {}'.format(file_path))\n",
        "\n",
        "\n",
        "  # create the json file from the dictionary\n",
        "  with open(json_path, 'w') as fp:\n",
        "    json.dump(data, fp, indent=4)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNV13MzS1-2M"
      },
      "source": [
        "**Step 5:** Run the function above with the desired parameters\n",
        "\n",
        "**As aforementioned, before running this, ensure that you have deleted the README.md file in the audio_data/_background_noise_ directory!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQKXmoGxjKJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "6e659322-bd12-4f73-de59-54dbbfd2f3f5"
      },
      "source": [
        "JSON_PATH = DATASET_ROOT_DIR + 'data.json'\n",
        "PROCESSING_METHOD = 'fft'\n",
        "SAMPLE_RATE = 16000\n",
        "EXPECTED_DURATION = 1 # in seconds\n",
        "WINDOW_SIZE_SAMPLES = 512\n",
        "WINDOW_STRIDE_SAMPLES = 320\n",
        "MFCC_COEFF_NUMBER = 13\n",
        "\n",
        "\n",
        "preprocess_entire_dataset(dataset_path = DATASET_ROOT_DIR, \n",
        "                   json_path = JSON_PATH, \n",
        "                   processing_method = PROCESSING_METHOD,\n",
        "                   sample_rate = SAMPLE_RATE, \n",
        "                   expected_duration = EXPECTED_DURATION, \n",
        "                   window_size = WINDOW_SIZE_SAMPLES, \n",
        "                   window_stride = WINDOW_STRIDE_SAMPLES, \n",
        "                   num_mfcc = MFCC_COEFF_NUMBER)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5c2d649338b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mJSON_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATASET_ROOT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'data.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPROCESSING_METHOD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fft'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mEXPECTED_DURATION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mWINDOW_SIZE_SAMPLES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATASET_ROOT_DIR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nio7zr1n-f17"
      },
      "source": [
        "### 3) Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz9DRkNBuhTF"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    print('loading data...')\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    X = np.array(data['features'])\n",
        "    y = np.array(data['labels'])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
        "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
        "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
        "    :return X_train (ndarray): Input training set\n",
        "    :return X_validation (ndarray): Input validation set\n",
        "    :return X_test (ndarray): Input test set\n",
        "    :return y_train (ndarray): Target training set\n",
        "    :return y_validation (ndarray): Target validation set\n",
        "    :return y_test (ndarray): Target test set\n",
        "    \"\"\"\n",
        "\n",
        "    print('preparing dataset...')\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(JSON_PATH)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "    print(X_train.shape)\n",
        "    # add an axis to input sets\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_validation = X_validation[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    \"\"\"Generates CNN model\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 1st conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # 2nd conv layer\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # # 3rd conv layer\n",
        "    # model.add(keras.layers.Conv2D(128, (2, 2), activation='relu'))\n",
        "    # model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
        "    # model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # flatten output and feed it into dense layer\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(37, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def predict(model, X, y):\n",
        "    \"\"\"Predict a single sample using the trained model\n",
        "    :param model: Trained classifier\n",
        "    :param X: Input data\n",
        "    :param y (int): Target\n",
        "    \"\"\"\n",
        "\n",
        "    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n",
        "    X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)\n",
        "\n",
        "    # perform prediction\n",
        "    prediction = model.predict(X)\n",
        "\n",
        "    # get index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)\n",
        "\n",
        "    print(\"Target: {}, Predicted label: {}\".format(y, predicted_index))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # get train, validation, test splits\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    # create network\n",
        "    print('X_train.shape: {}'.format(X_train.shape))\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimiser,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n",
        "\n",
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Ay09Hr1CzD"
      },
      "source": [
        "model.save(\"all_words_fft.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UzQyblHauQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f3f2fd-c0d8-47a0-ddeb-e39438d20807"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# pick a sample to predict from the test set\n",
        "X_to_predict = X_test[100]\n",
        "y_to_predict = y_test[100]\n",
        "\n",
        "# predict sample\n",
        "predict(model, X_to_predict, y_to_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "827/827 - 6s - loss: 0.3152 - accuracy: 0.9076\n",
            "\n",
            "Test accuracy: 0.907555103302002\n",
            "Target: 18, Predicted label: [18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEQYCXLUbbSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac7a597-353f-4daf-f93a-48f30629a846"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "def representative_dataset_generator():\n",
        "  for value in X_test:\n",
        "    yield [np.array(value, dtype = np.float32, ndmin=4)]\n",
        "\n",
        "converter.representative_dataset = representative_dataset_generator\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"all_words_fft.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"all_words_fft.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpiw9ksaq5/assets\n",
            "Model is 420788 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXCDA0gobx2g"
      },
      "source": [
        "# !echo \"const unsigned char model[] __attribute__((aligned(4))) = {\"  > /content/model.h\n",
        "# !cat model1.tflite | xxd -i                                  >> /content/model.h\n",
        "# !echo \"};\"                                                          >> /content/model.h\n",
        "\n",
        "# import os\n",
        "# model_h_size = os.path.getsize(\"model.h\")\n",
        "# print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "# print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClYTyCuld02O"
      },
      "source": [
        "# Function: Convert some hex value into an array for C programming\n",
        "def hex_to_c_array(hex_data, var_name):\n",
        "\n",
        "  c_str = ''\n",
        "\n",
        "  # Create header guard\n",
        "  c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
        "  c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
        "\n",
        "  # Add array length at top of file\n",
        "  c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
        "\n",
        "  # Declare C variable\n",
        "  c_str += 'unsigned char ' + var_name + '[] = {'\n",
        "  hex_array = []\n",
        "  for i, val in enumerate(hex_data) :\n",
        "\n",
        "    # Construct string from hex\n",
        "    hex_str = format(val, '#04x')\n",
        "\n",
        "    # Add formatting so each line stays within 80 characters\n",
        "    if (i + 1) < len(hex_data):\n",
        "      hex_str += ','\n",
        "    if (i + 1) % 12 == 0:\n",
        "      hex_str += '\\n '\n",
        "    hex_array.append(hex_str)\n",
        "\n",
        "  # Add closing brace\n",
        "  c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
        "\n",
        "  # Close out header guard\n",
        "  c_str += '#endif //' + var_name.upper() + '_H'\n",
        "\n",
        "  return c_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1I1PIs74r1P"
      },
      "source": [
        "with open(\"all_words\" + '.h', 'w') as file:\n",
        "  file.write(hex_to_c_array(tflite_model, \"all_words\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}