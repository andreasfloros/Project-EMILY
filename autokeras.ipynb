{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autokeras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCpWZRW5E1w8wm8yJ8yzaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreasfloros/ARM-ML-Embedded/blob/main/autokeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xNjJ3_12C73"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "DATASET_ROOT_DIR = 'audio_data/'\n",
        "os.mkdir(DATASET_ROOT_DIR)\n",
        "url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "target_path = 'audio_data/dataset.tar.gz'\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(target_path, 'wb') as f:\n",
        "        f.write(response.raw.read())\n",
        "\n",
        "tar = tarfile.open(target_path, \"r:gz\")\n",
        "tar.extractall(path='audio_data/')\n",
        "tar.close()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCUii9Oe2GmC"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import math\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izb_V3B_2KSk"
      },
      "source": [
        "def make_track_correct_size(signal, expected_num_samples_per_track):\n",
        "\n",
        "    # print('Original track length: {}'.format(len(signal)))\n",
        "    # if track is shorter than expected, append it with zeros\n",
        "    if len(signal) < expected_num_samples_per_track:\n",
        "      num_zeros_to_pad = expected_num_samples_per_track - len(signal)\n",
        "      zeros = num_zeros_to_pad * [0.]\n",
        "      extended_signal = np.append(signal, zeros)\n",
        "      return extended_signal\n",
        "\n",
        "    # if track is longer than expected, truncate it\n",
        "    elif len(signal) > expected_num_samples_per_track:\n",
        "      return signal[:expected_num_samples_per_track]\n",
        "\n",
        "    # else return the original track \n",
        "    else:\n",
        "      return signal"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSY5UKl82NTo"
      },
      "source": [
        "def audio_track_to_features(signal, processing_method, sample_rate, window_size, window_stride, num_mfcc):\n",
        "  \n",
        "  if processing_method == 'fft':\n",
        "    # perform Fast Fourier Transform (FFT)\n",
        "    fft = np.fft.fft(signal)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrum = np.abs(fft)\n",
        "\n",
        "    # the spectrum is symmetrical with respect to sample_rate / 2\n",
        "    # so take half of the spectrum and frequency arrays\n",
        "    # therefore len(half_spectrum) = sample_rate / 2\n",
        "    half_spectrum = spectrum[:int(len(spectrum)/2)]\n",
        "\n",
        "    # average every 16 samples to reduce size of array to 1 / 16 of its original size\n",
        "    # e.g. sample_rate = 16k, duration = 1.024s, reduce size from 8192 to 512 \n",
        "    averaged = np.mean(half_spectrum.reshape(-1, 16), axis=1)\n",
        "    return averaged\n",
        "\n",
        "\n",
        "  elif processing_method == 'stft':\n",
        "    # perform Short Time Fourier Transform (STFT)\n",
        "    stft = librosa.stft(signal = signal, \n",
        "                        n_fft = window_size, \n",
        "                        hop_length = window_stride)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrogram = np.abs(stft)\n",
        "\n",
        "    # transpose and return the spectrogram matrix\n",
        "    transposed_spectrogram = spectrogram.transpose()\n",
        "    return transposed_spectrogram.flatten()\n",
        "\n",
        "\n",
        "  else: # mfcc\n",
        "    # perform Mel-Frequency Cepstral Coefficients (MFCC)\n",
        "    mfcc = librosa.feature.mfcc(signal, \n",
        "                                sr = sample_rate, \n",
        "                                n_fft = window_size, \n",
        "                                n_mfcc = num_mfcc,\n",
        "                                hop_length = window_stride)\n",
        "    # transpose and return the mfcc matrix\n",
        "    transposed_mfcc = mfcc.T\n",
        "    return transposed_mfcc.flatten()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9y3B21e2Pkn"
      },
      "source": [
        "def preprocess_entire_dataset(dataset_path, json_path, processing_method, sample_rate, expected_duration, window_size, window_stride, num_mfcc):\n",
        "  # expected duration is in seconds\n",
        "  expected_num_samples_per_track = int(expected_duration * sample_rate)\n",
        "  \n",
        "  # dictionary to later be converted to final json file\n",
        "  data = {\n",
        "      'mapping' : [],\n",
        "      'features' : [],\n",
        "      'labels' : []\n",
        "  }\n",
        "\n",
        "  # we will iterate this for each of the visited sub-directorie in order to\n",
        "  # give a different label for each of them\n",
        "  visited_directory_index = 0\n",
        "\n",
        "  # iterate through all subfolders\n",
        "  for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
        "\n",
        "    # # ensure we are not at the dataset root directory\n",
        "    # # (os.walk provides this directory as well)\n",
        "    if dirpath is not DATASET_ROOT_DIR:\n",
        "    # if dirpath == 'audio_data/yes' or dirpath == 'audio_data/no':\n",
        "\n",
        "      # obtain word labels\n",
        "      dirpath_components = dirpath.split('/') # audio_data/left => ['audio_data', 'left']\n",
        "      word_label = dirpath_components[-1]\n",
        "      data['mapping'].append(word_label)\n",
        "      print('Processing {}'.format(word_label))\n",
        "\n",
        "      # access and process files for current word\n",
        "      for f in filenames:\n",
        "        \n",
        "        # load audio file\n",
        "        file_path = os.path.join(dirpath, f)\n",
        "        signal, sample_rate = librosa.load(file_path, sr=sample_rate)\n",
        "\n",
        "        # extend or cut signal to be equal to the expected size\n",
        "        signal_correct_size = make_track_correct_size(signal, expected_num_samples_per_track)\n",
        "\n",
        "        # obtain the features of the audio track using the function defined above\n",
        "        track_features = audio_track_to_features(signal = signal_correct_size, \n",
        "                                                 processing_method = processing_method,\n",
        "                                                 sample_rate = sample_rate, \n",
        "                                                 window_size = window_size, \n",
        "                                                 window_stride = window_stride, \n",
        "                                                 num_mfcc = 13)\n",
        "        \n",
        "        # append the audio track features to the features field of the dictionary\n",
        "        data['features'].append(track_features.tolist())\n",
        "\n",
        "        # append the current directory index as the label of this track\n",
        "        data['labels'].append(visited_directory_index)\n",
        "        # print('file_path: {}'.format(file_path))\n",
        "\n",
        "      # iterate the index before visiting the next directory\n",
        "      visited_directory_index = visited_directory_index + 1\n",
        "\n",
        "  print(data['mapping'])\n",
        "  print(set(data['labels']))\n",
        "  # create the json file from the dictionary\n",
        "  with open(json_path, 'w') as fp:\n",
        "    json.dump(data, fp, indent=4)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ50Y9zJ2Rd1"
      },
      "source": [
        "rm -rf `find -type d -name .ipynb_checkpoints`"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH9VWheB2Sy4"
      },
      "source": [
        "!rm audio_data/_background_noise_/README.md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxWiWhl62UkB"
      },
      "source": [
        "JSON_PATH = 'data.json'\n",
        "PROCESSING_METHOD = 'mfcc'\n",
        "SAMPLE_RATE = 16000\n",
        "EXPECTED_DURATION = 1.024            # in seconds\n",
        "WINDOW_SIZE_SAMPLES = 512            # in samples\n",
        "WINDOW_STRIDE_SAMPLES = 320          # in samples\n",
        "MFCC_COEFF_NUMBER = 13\n",
        "\n",
        "preprocess_entire_dataset(dataset_path = DATASET_ROOT_DIR, \n",
        "                   json_path = JSON_PATH, \n",
        "                   processing_method = PROCESSING_METHOD,\n",
        "                   sample_rate = SAMPLE_RATE, \n",
        "                   expected_duration = EXPECTED_DURATION, \n",
        "                   window_size = WINDOW_SIZE_SAMPLES, \n",
        "                   window_stride = WINDOW_STRIDE_SAMPLES, \n",
        "                   num_mfcc = MFCC_COEFF_NUMBER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUMs8783yss"
      },
      "source": [
        "Import libraries for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2io5ne8u7MtT"
      },
      "source": [
        "!pip install autokeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Bq2jmb2ZFq"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Reshape, Activation, BatchNormalization, Conv2D, MaxPooling2D, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "import autokeras as ak\n",
        "import kerastuner as kt"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuO4KC753wtT"
      },
      "source": [
        "Load the data and prepare datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FVsHXdH3tw5"
      },
      "source": [
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    print('Loading dataset')\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    X = np.array(data['features'])\n",
        "    y = np.array(data['labels'])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
        "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
        "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
        "    :return X_train (ndarray): Input training set\n",
        "    :return X_validation (ndarray): Input validation set\n",
        "    :return X_test (ndarray): Input test set\n",
        "    :return y_train (ndarray): Target training set\n",
        "    :return y_validation (ndarray): Target validation set\n",
        "    :return y_test (ndarray): Target test set\n",
        "    \"\"\"\n",
        "\n",
        "    print('Splitting dataset into training, validation, and test splits')\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(JSON_PATH)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    # X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # add an axis to input sets for conv networks to fit conv2D shape specs\n",
        "    # currently not needed since we reshape everything anyway\n",
        "    # X_train = X_train[..., np.newaxis]\n",
        "    # X_validation = X_validation[..., np.newaxis]\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8tnRKJeAM56"
      },
      "source": [
        "X_train, X_test, y_train, y_test = prepare_datasets(0.25, 0.2)\n",
        "print('Finished preparing training, validation, and test data')\n",
        "print('X_train.shape: {}'.format(X_train.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FUDy8T84Qgu"
      },
      "source": [
        "Build AutoKera Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm8I-Tlv_Dm1",
        "outputId": "0b45e05d-9d8e-49e7-debc-caac4bf2d42a"
      },
      "source": [
        "hp = kt.HyperParameters()\n",
        "num_units = hp.Choice(\"num_units\", values=[16,32,64])\n",
        "input_node = ak.Input()\n",
        "output_node = ak.DenseBlock()(input_node)\n",
        "output_node = ak.ClassificationHead(num_classes=36, loss = \"categorical_crossentropy\")(output_node)\n",
        "auto_model = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, objective=\"accuracy\", tuner=\"greedy\", overwrite=True, max_trials=10\n",
        ")\n",
        "auto_model.fit(X_train, y_train, epochs=10)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 01m 28s]\n",
            "accuracy: 0.7838003635406494\n",
            "\n",
            "Best accuracy So Far: 0.7838003635406494\n",
            "Total elapsed time: 00h 11m 28s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/10\n",
            "2481/2481 [==============================] - 10s 3ms/step - loss: 3.1792 - accuracy: 0.1637\n",
            "Epoch 2/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.5075 - accuracy: 0.5591\n",
            "Epoch 3/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 1.2067 - accuracy: 0.6423\n",
            "Epoch 4/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.0683 - accuracy: 0.6821\n",
            "Epoch 5/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 0.9757 - accuracy: 0.7081\n",
            "Epoch 6/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.9055 - accuracy: 0.7289\n",
            "Epoch 7/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.8470 - accuracy: 0.7462\n",
            "Epoch 8/10\n",
            "2481/2481 [==============================] - 8s 3ms/step - loss: 0.7958 - accuracy: 0.7614\n",
            "Epoch 9/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 0.7515 - accuracy: 0.7739\n",
            "Epoch 10/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.7143 - accuracy: 0.7856\n",
            "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmjqELmxDdw7",
        "outputId": "1ec1b86a-04f8-4af4-a0f4-0cc6a68aa87e"
      },
      "source": [
        "print(auto_model.evaluate(X_test, y_test))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "827/827 [==============================] - 2s 2ms/step - loss: 0.8010 - accuracy: 0.7621\n",
            "[0.800967276096344, 0.7621225118637085]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCc3gtf3FpB5",
        "outputId": "f8a199de-7097-4df7-f282-5b37de17ea25"
      },
      "source": [
        "model = auto_model.export_model()\n",
        "print(type(model))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.keras.engine.functional.Functional'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1mXImT2FzXF",
        "outputId": "493f6225-2037-4926-b434-87a17df332a2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 676)]             0         \n",
            "_________________________________________________________________\n",
            "cast_to_float32 (CastToFloat (None, 676)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                21664     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               16896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 36)                1188      \n",
            "_________________________________________________________________\n",
            "classification_head_2 (Softm (None, 36)                0         \n",
            "=================================================================\n",
            "Total params: 58,468\n",
            "Trainable params: 57,316\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}