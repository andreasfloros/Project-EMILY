{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autokeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOb9MbstvA+Na7WTd/vVOH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreasfloros/ARM-ML-Embedded/blob/main/autokeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RqHhQ2B2ZPh"
      },
      "source": [
        "## Prepare and preprocess data\n",
        "Referenced from audio_classifier pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RhgqzAxg4at",
        "outputId": "1c56eab0-f1cf-4c57-a79f-0d65cf680bc8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO_nAYsFE33I"
      },
      "source": [
        "DATASET_ROOT_DIR = '/gdrive/MyDrive/audio_data/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "-xNjJ3_12C73",
        "outputId": "13d0977d-efc8-4047-d0c6-4e80b28d1a81"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "\n",
        "os.mkdir(DATASET_ROOT_DIR)\n",
        "url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "target_path = 'audio_data/dataset.tar.gz'\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(target_path, 'wb') as f:\n",
        "        f.write(response.raw.read())\n",
        "\n",
        "tar = tarfile.open(target_path, \"r:gz\")\n",
        "tar.extractall(path='audio_data/')\n",
        "tar.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1d4ed88f9ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_ROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'audio_data/dataset.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/gdrive/MyDrive/audio_data/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCUii9Oe2GmC"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import math\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izb_V3B_2KSk"
      },
      "source": [
        "def make_track_correct_size(signal, expected_num_samples_per_track):\n",
        "\n",
        "    # print('Original track length: {}'.format(len(signal)))\n",
        "    # if track is shorter than expected, append it with zeros\n",
        "    if len(signal) < expected_num_samples_per_track:\n",
        "      num_zeros_to_pad = expected_num_samples_per_track - len(signal)\n",
        "      zeros = num_zeros_to_pad * [0.]\n",
        "      extended_signal = np.append(signal, zeros)\n",
        "      return extended_signal\n",
        "\n",
        "    # if track is longer than expected, truncate it\n",
        "    elif len(signal) > expected_num_samples_per_track:\n",
        "      return signal[:expected_num_samples_per_track]\n",
        "\n",
        "    # else return the original track \n",
        "    else:\n",
        "      return signal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSY5UKl82NTo"
      },
      "source": [
        "def audio_track_to_features(signal, processing_method, sample_rate, window_size, window_stride, num_mfcc):\n",
        "  \n",
        "  if processing_method == 'fft':\n",
        "    # perform Fast Fourier Transform (FFT)\n",
        "    fft = np.fft.fft(signal)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrum = np.abs(fft)\n",
        "\n",
        "    # the spectrum is symmetrical with respect to sample_rate / 2\n",
        "    # so take half of the spectrum and frequency arrays\n",
        "    # therefore len(half_spectrum) = sample_rate / 2\n",
        "    half_spectrum = spectrum[:int(len(spectrum)/2)]\n",
        "\n",
        "    # average every 16 samples to reduce size of array to 1 / 16 of its original size\n",
        "    # e.g. sample_rate = 16k, duration = 1.024s, reduce size from 8192 to 512 \n",
        "    averaged = np.mean(half_spectrum.reshape(-1, 16), axis=1)\n",
        "    return averaged\n",
        "\n",
        "\n",
        "  elif processing_method == 'stft':\n",
        "    # perform Short Time Fourier Transform (STFT)\n",
        "    stft = librosa.stft(signal = signal, \n",
        "                        n_fft = window_size, \n",
        "                        hop_length = window_stride)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrogram = np.abs(stft)\n",
        "\n",
        "    # transpose and return the spectrogram matrix\n",
        "    transposed_spectrogram = spectrogram.transpose()\n",
        "    return transposed_spectrogram.flatten()\n",
        "\n",
        "\n",
        "  else: # mfcc\n",
        "    # perform Mel-Frequency Cepstral Coefficients (MFCC)\n",
        "    mfcc = librosa.feature.mfcc(signal, \n",
        "                                sr = sample_rate, \n",
        "                                n_fft = window_size, \n",
        "                                n_mfcc = num_mfcc,\n",
        "                                hop_length = window_stride)\n",
        "    # transpose and return the mfcc matrix\n",
        "    transposed_mfcc = mfcc.T\n",
        "    return transposed_mfcc.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9y3B21e2Pkn"
      },
      "source": [
        "def preprocess_entire_dataset(dataset_path, json_path, processing_method, sample_rate, expected_duration, window_size, window_stride, num_mfcc):\n",
        "  # expected duration is in seconds\n",
        "  expected_num_samples_per_track = int(expected_duration * sample_rate)\n",
        "  \n",
        "  # dictionary to later be converted to final json file\n",
        "  data = {\n",
        "      'mapping' : [],\n",
        "      'features' : [],\n",
        "      'labels' : []\n",
        "  }\n",
        "\n",
        "  # we will iterate this for each of the visited sub-directorie in order to\n",
        "  # give a different label for each of them\n",
        "  visited_directory_index = 0\n",
        "\n",
        "  # iterate through all subfolders\n",
        "  for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
        "\n",
        "    # # ensure we are not at the dataset root directory\n",
        "    # # (os.walk provides this directory as well)\n",
        "    if dirpath is not DATASET_ROOT_DIR:\n",
        "    # if dirpath == 'audio_data/yes' or dirpath == 'audio_data/no':\n",
        "\n",
        "      # obtain word labels\n",
        "      dirpath_components = dirpath.split('/') # audio_data/left => ['audio_data', 'left']\n",
        "      word_label = dirpath_components[-1]\n",
        "      data['mapping'].append(word_label)\n",
        "      print('Processing {}'.format(word_label))\n",
        "\n",
        "      # access and process files for current word\n",
        "      for f in filenames:\n",
        "        \n",
        "        # load audio file\n",
        "        file_path = os.path.join(dirpath, f)\n",
        "        signal, sample_rate = librosa.load(file_path, sr=sample_rate)\n",
        "\n",
        "        # extend or cut signal to be equal to the expected size\n",
        "        signal_correct_size = make_track_correct_size(signal, expected_num_samples_per_track)\n",
        "\n",
        "        # obtain the features of the audio track using the function defined above\n",
        "        track_features = audio_track_to_features(signal = signal_correct_size, \n",
        "                                                 processing_method = processing_method,\n",
        "                                                 sample_rate = sample_rate, \n",
        "                                                 window_size = window_size, \n",
        "                                                 window_stride = window_stride, \n",
        "                                                 num_mfcc = 13)\n",
        "        \n",
        "        # append the audio track features to the features field of the dictionary\n",
        "        data['features'].append(track_features.tolist())\n",
        "\n",
        "        # append the current directory index as the label of this track\n",
        "        data['labels'].append(visited_directory_index)\n",
        "        # print('file_path: {}'.format(file_path))\n",
        "\n",
        "      # iterate the index before visiting the next directory\n",
        "      visited_directory_index = visited_directory_index + 1\n",
        "\n",
        "  print(data['mapping'])\n",
        "  print(set(data['labels']))\n",
        "  # create the json file from the dictionary\n",
        "  with open(json_path, 'w') as fp:\n",
        "    json.dump(data, fp, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ50Y9zJ2Rd1"
      },
      "source": [
        "rm -rf `find -type d -name .ipynb_checkpoints`"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH9VWheB2Sy4",
        "outputId": "d9bea4c7-8022-4eaa-e28a-4e529af47110"
      },
      "source": [
        "!rm /gdrive/MyDrive/audio_data/_background_noise_/README.md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/gdrive/MyDrive/audio_data/_background_noise_/README.md': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlWF8Hm05JVU"
      },
      "source": [
        "JSON_PATH = DATASET_ROOT_DIR+'data.json'\n",
        "PROCESSING_METHOD = 'mfcc'\n",
        "SAMPLE_RATE = 16000\n",
        "EXPECTED_DURATION = 1.024            # in seconds\n",
        "WINDOW_SIZE_SAMPLES = 512            # in samples\n",
        "WINDOW_STRIDE_SAMPLES = 320          # in samples\n",
        "MFCC_COEFF_NUMBER = 13"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "hxWiWhl62UkB",
        "outputId": "cf492eb7-6569-4508-b3eb-08661e4498a8"
      },
      "source": [
        "preprocess_entire_dataset(dataset_path = DATASET_ROOT_DIR, \n",
        "                   json_path = JSON_PATH, \n",
        "                   processing_method = PROCESSING_METHOD,\n",
        "                   sample_rate = SAMPLE_RATE, \n",
        "                   expected_duration = EXPECTED_DURATION, \n",
        "                   window_size = WINDOW_SIZE_SAMPLES, \n",
        "                   window_stride = WINDOW_STRIDE_SAMPLES, \n",
        "                   num_mfcc = MFCC_COEFF_NUMBER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing dog\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-60bccd70acf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                    \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWINDOW_SIZE_SAMPLES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                    \u001b[0mwindow_stride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWINDOW_STRIDE_SAMPLES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                    num_mfcc = MFCC_COEFF_NUMBER)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-9c0fed1e72e0>\u001b[0m in \u001b[0;36mpreprocess_entire_dataset\u001b[0;34m(dataset_path, json_path, processing_method, sample_rate, expected_duration, window_size, window_stride, num_mfcc)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# load audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# extend or cut signal to be equal to the expected size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfilesystemencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_open_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUMs8783yss"
      },
      "source": [
        "Import libraries for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2io5ne8u7MtT",
        "outputId": "907cce7e-7f10-4d4e-abfe-4385414de8ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install autokeras"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autokeras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/12/cf698586ccc8245f08d1843dcafb65b064a2e9e2923b889dc58e1019f099/autokeras-1.0.12-py3-none-any.whl (164kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 26.4MB/s \n",
            "\u001b[?25hCollecting keras-tuner>=1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from autokeras) (20.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autokeras) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from autokeras) (2.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->autokeras) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.0.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.32.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.36.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2.10)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.30.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.4.8)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=aa1b205a5d84aafffc1dbcad67ab72d66fd3cc809e6907cb50d0543a1587033f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=d7bee1bbe9b8b47d3abcb83802dd6e19889fc17bd4f0d716ed7cf65c0dce8861\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner, autokeras\n",
            "Successfully installed autokeras-1.0.12 colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Bq2jmb2ZFq"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Reshape, Activation, BatchNormalization, Conv2D, MaxPooling2D, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "import autokeras as ak\n",
        "import kerastuner as kt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuO4KC753wtT"
      },
      "source": [
        "Load the data and prepare datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FVsHXdH3tw5"
      },
      "source": [
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    print('Loading dataset')\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    X = np.array(data['features'])\n",
        "    y = np.array(data['labels'])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
        "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
        "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
        "    :return X_train (ndarray): Input training set\n",
        "    :return X_validation (ndarray): Input validation set\n",
        "    :return X_test (ndarray): Input test set\n",
        "    :return y_train (ndarray): Target training set\n",
        "    :return y_validation (ndarray): Target validation set\n",
        "    :return y_test (ndarray): Target test set\n",
        "    \"\"\"\n",
        "\n",
        "    print('Splitting dataset into training, validation, and test splits')\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(JSON_PATH)\n",
        "\n",
        "    # if flatten == False:\n",
        "    #   Reshape(reshape_shape, input_shape=input_shape)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    # X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # add an axis to input sets for conv networks to fit conv2D shape specs\n",
        "    # currently not needed since we reshape everything anyway\n",
        "    # X_train = X_train[..., np.newaxis]\n",
        "    # X_validation = X_validation[..., np.newaxis]\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8tnRKJeAM56",
        "outputId": "5813e289-8930-403f-a80c-0d8c7df73894"
      },
      "source": [
        "X_train, X_test, y_train, y_test = prepare_datasets(0.25, 0.2)\n",
        "print('Finished preparing training, validation, and test data')\n",
        "print('X_train.shape: {}'.format(X_train.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting dataset into training, validation, and test splits\n",
            "Loading dataset\n",
            "Finished preparing training, validation, and test data\n",
            "X_train.shape: (79376, 676)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib02XWmS1KwL",
        "outputId": "edc72848-1857-454d-ae6a-167dabfb2ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "X_train.type"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-197a1eb9c09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZwhhJTYwfal",
        "outputId": "7cf7f300-e335-494d-b030-5251a15fb239"
      },
      "source": [
        "NUM_OF_SAMPLES = SAMPLE_RATE * EXPECTED_DURATION\n",
        "NUM_OF_WINDOW_POSITIONS = math.ceil(NUM_OF_SAMPLES / WINDOW_STRIDE_SAMPLES)\n",
        "NUM_OF_STFT_FREQUENCIES = int(1 + WINDOW_SIZE_SAMPLES / 2)\n",
        "\n",
        "FFT_ARRAY_SIZE = int((NUM_OF_SAMPLES) / (2 * 16))  # 2 comes from half spectrum, 16 from averaging every 16 samples\n",
        "\n",
        "if PROCESSING_METHOD == 'fft':\n",
        "  RESHAPE_SHAPE = (1, FFT_ARRAY_SIZE, 1)\n",
        "elif PROCESSING_METHOD == 'stft':\n",
        "  RESHAPE_SHAPE = (NUM_OF_WINDOW_POSITIONS, NUM_OF_STFT_FREQUENCIES, 1)\n",
        "else: # mfcc\n",
        "  RESHAPE_SHAPE = (NUM_OF_WINDOW_POSITIONS, MFCC_COEFF_NUMBER, 1)\n",
        "\n",
        "print('RESHAPE_SHAPE: {}'.format(RESHAPE_SHAPE))\n",
        "INPUT_SHAPE = X_train.shape[1:]\n",
        "print('INPUT_SHAPE:{}'.format(INPUT_SHAPE))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RESHAPE_SHAPE: (52, 13, 1)\n",
            "INPUT_SHAPE:(676,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FUDy8T84Qgu"
      },
      "source": [
        "Build AutoKera Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Xa-IeZ3fva"
      },
      "source": [
        "class SingleLayerReshapeBlock(ak.Block):\n",
        "  def __init__(self, input_shape, reshape_shape):\n",
        "    super().__init__()\n",
        "    self.input_shape = input_shape\n",
        "    self.reshape_shape = reshape_shape\n",
        "\n",
        "  def build(self, hp, inputs=None):\n",
        "      # Get the input_node from inputs.\n",
        "    input_node = tf.nest.flatten(inputs)[0]\n",
        "    layer = Reshape(self.reshape_shape, input_shape=self.input_shape)\n",
        "    output_node = layer(input_node)\n",
        "    return output_node\n",
        "\n",
        "  # def __call__(self, *args, **kwargs):\n",
        "  #   return self.build(*args, **kwargs)\n",
        "\n",
        "def build_conv_model():\n",
        "  hp = kt.HyperParameters()\n",
        "  filters = hp.Choice(\"filters\", values=[16,32,64])\n",
        "\n",
        "  input_node = ak.Input()\n",
        "  output_node = SingleLayerReshapeBlock(INPUT_SHAPE, RESHAPE_SHAPE)(input_node)\n",
        "  output_node = ak.ConvBlock(kernel_size=3,filters=filters,num_layers=1, separable=True, max_pooling=True)(output_node)\n",
        "  output_node = ak.ClassificationHead()(output_node)\n",
        "  auto_model = ak.AutoModel(\n",
        "      inputs=input_node, outputs=output_node, objective=\"accuracy\", tuner=\"greedy\", overwrite=True, max_trials=20, max_model_size=60000\n",
        "  )\n",
        "  auto_model.fit(X_train, y_train, epochs=10)\n",
        "  return auto_model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n656I6A62aP",
        "outputId": "59c1642c-3279-4570-9fe0-bd2c519d14cb"
      },
      "source": [
        "auto_model = build_conv_model()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 20 Complete [00h 01m 32s]\n",
            "accuracy: 0.3372953534126282\n",
            "\n",
            "Best accuracy So Far: 0.7040774822235107\n",
            "Total elapsed time: 00h 29m 46s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 2.8536 - accuracy: 0.2082\n",
            "Epoch 2/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.7418 - accuracy: 0.4916\n",
            "Epoch 3/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.5445 - accuracy: 0.5462\n",
            "Epoch 4/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.4336 - accuracy: 0.5791\n",
            "Epoch 5/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.3629 - accuracy: 0.5984\n",
            "Epoch 6/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.3059 - accuracy: 0.6139\n",
            "Epoch 7/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.2581 - accuracy: 0.6269\n",
            "Epoch 8/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.2194 - accuracy: 0.6381\n",
            "Epoch 9/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.1873 - accuracy: 0.6469\n",
            "Epoch 10/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.1582 - accuracy: 0.6547\n",
            "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T_T27FV3zDC",
        "outputId": "20d8db8a-b749-4fa0-9483-b69d5b86f6ac"
      },
      "source": [
        "print(auto_model.evaluate(X_test, y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "827/827 [==============================] - 2s 2ms/step - loss: 1.1623 - accuracy: 0.6577\n",
            "[1.1622689962387085, 0.6576589941978455]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK2sfsWI31Po",
        "outputId": "1e638691-b16d-492d-d5c7-9de856f629e8"
      },
      "source": [
        "model = auto_model.export_model()\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 676)]             0         \n",
            "_________________________________________________________________\n",
            "cast_to_float32 (CastToFloat (None, 676)               0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 52, 13, 1)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 50, 11, 16)        41        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 25, 5, 16)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_1 (Separabl (None, 25, 5, 16)         416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_2 (Separabl (None, 12, 2, 16)         416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 1, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 36)                612       \n",
            "_________________________________________________________________\n",
            "classification_head_1 (Softm (None, 36)                0         \n",
            "=================================================================\n",
            "Total params: 1,485\n",
            "Trainable params: 1,485\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDU2vkfwKYI2"
      },
      "source": [
        "model_name = 'all_words_mfcc'\n",
        "model.save(model_name+'.h5')\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY0rtQT-j6sG"
      },
      "source": [
        "loaded_model = keras.models.load_model('all_words_mfcc.h5')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf3ciLsZkROQ",
        "outputId": "f1d1b872-ffeb-4f01-9e16-5ea4ca712fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loaded_model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 676)]             0         \n",
            "_________________________________________________________________\n",
            "cast_to_float32 (CastToFloat (None, 676)               0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 52, 13, 1)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 50, 11, 16)        41        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 25, 5, 16)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_1 (Separabl (None, 25, 5, 16)         416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_2 (Separabl (None, 12, 2, 16)         416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 1, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 36)                612       \n",
            "_________________________________________________________________\n",
            "classification_head_1 (Softm (None, 36)                0         \n",
            "=================================================================\n",
            "Total params: 1,485\n",
            "Trainable params: 1,485\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5wstSHskUjK",
        "outputId": "8901f1b1-3e72-4186-b7ca-fedaffb05ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "# Save the model to disk\n",
        "open(model_name + '.tflite', \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(model_name + '.tflite')\n",
        "print(\"Model is %d bytes\" % basic_model_size)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxswkq7l0/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxswkq7l0/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing table_initializer.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Issue encountered when serializing table_initializer.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ConverterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    212\u001b[0m                                                  \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                                  enable_mlir_converter)\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/wrap_toco.py\u001b[0m in \u001b[0;36mwrapped_toco_convert\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       enable_mlir_converter)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1205:0: error: 'tf.Cast' op is neither a custom op nor a flex op\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1299:0: note: called from\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py:847:0: note: called from\n<ipython-input-32-59eff40c545b>:2:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\n\ttf.Cast {Truncate = false, device = \"\"}\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-59eff40c545b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tflite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m     return super(TFLiteKerasModelConverterV2,\n\u001b[0;32m--> 874\u001b[0;31m                  self).convert(graph_def, input_tensors, output_tensors)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         **converter_kwargs)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0mcalibrate_and_quantize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[1;32m    614\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    214\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_toco_from_proto_bin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConverterError\u001b[0m: /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1205:0: error: 'tf.Cast' op is neither a custom op nor a flex op\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1299:0: note: called from\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py:847:0: note: called from\n<ipython-input-32-59eff40c545b>:2:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\n\ttf.Cast {Truncate = false, device = \"\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a14u1RuXhWre",
        "outputId": "c89d3164-78c0-4a24-b792-81d812fba52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# def representative_dataset_generator():\n",
        "#   for value in X_test:\n",
        "#     yield [np.array(value, dtype = np.float32, ndmin=4)]\n",
        "\n",
        "# converter.representative_dataset = representative_dataset_generator\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(model_name + '.tflite', \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(model_name + '.tflite')\n",
        "print(\"Model is %d bytes\" % basic_model_size)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp3pmrssvj/assets\n",
            "WARNING:tensorflow:Issue encountered when serializing table_initializer.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ConverterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    212\u001b[0m                                                  \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                                  enable_mlir_converter)\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/wrap_toco.py\u001b[0m in \u001b[0;36mwrapped_toco_convert\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       enable_mlir_converter)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1205:0: error: 'tf.Cast' op is neither a custom op nor a flex op\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1299:0: note: called from\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py:847:0: note: called from\n<ipython-input-26-f24fc9d57adf>:13:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\n\ttf.Cast {Truncate = false, device = \"\"}\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f24fc9d57adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# converter.representative_dataset = representative_dataset_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Save the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m     return super(TFLiteKerasModelConverterV2,\n\u001b[0;32m--> 874\u001b[0;31m                  self).convert(graph_def, input_tensors, output_tensors)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m         **converter_kwargs)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0mcalibrate_and_quantize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[1;32m    614\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    214\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_toco_from_proto_bin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConverterError\u001b[0m: /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1205:0: error: 'tf.Cast' op is neither a custom op nor a flex op\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1299:0: note: called from\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py:847:0: note: called from\n<ipython-input-26-f24fc9d57adf>:13:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\n/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\n/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\n\ttf.Cast {Truncate = false, device = \"\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm8I-Tlv_Dm1",
        "outputId": "0b45e05d-9d8e-49e7-debc-caac4bf2d42a"
      },
      "source": [
        "def build_dense_model():\n",
        "  hp = kt.HyperParameters()\n",
        "  num_units = hp.Choice(\"num_units\", values=[16,32,64,128,256])\n",
        "  input_node = ak.Input()\n",
        "  output_node = ak.DenseBlock(num_units=num_units)(input_node)\n",
        "  output_node = ak.ClassificationHead(num_classes=36, loss = \"categorical_crossentropy\")(output_node)\n",
        "  auto_model = ak.AutoModel(\n",
        "      inputs=input_node, outputs=output_node, objective=\"accuracy\", tuner=\"greedy\", overwrite=True, max_trials=10\n",
        "  )\n",
        "  auto_model.fit(X_train, y_train, epochs=10)\n",
        "  return auto_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 01m 28s]\n",
            "accuracy: 0.7838003635406494\n",
            "\n",
            "Best accuracy So Far: 0.7838003635406494\n",
            "Total elapsed time: 00h 11m 28s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/10\n",
            "2481/2481 [==============================] - 10s 3ms/step - loss: 3.1792 - accuracy: 0.1637\n",
            "Epoch 2/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.5075 - accuracy: 0.5591\n",
            "Epoch 3/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 1.2067 - accuracy: 0.6423\n",
            "Epoch 4/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.0683 - accuracy: 0.6821\n",
            "Epoch 5/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 0.9757 - accuracy: 0.7081\n",
            "Epoch 6/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.9055 - accuracy: 0.7289\n",
            "Epoch 7/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.8470 - accuracy: 0.7462\n",
            "Epoch 8/10\n",
            "2481/2481 [==============================] - 8s 3ms/step - loss: 0.7958 - accuracy: 0.7614\n",
            "Epoch 9/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 0.7515 - accuracy: 0.7739\n",
            "Epoch 10/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.7143 - accuracy: 0.7856\n",
            "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4qzykVlSoK-",
        "outputId": "fcbf5c4a-177c-41cb-91f9-ae495b45d179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "auto_model = build_dense_model()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-715ca564dc36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauto_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dense_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'build_dense_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmjqELmxDdw7",
        "outputId": "1ec1b86a-04f8-4af4-a0f4-0cc6a68aa87e"
      },
      "source": [
        "print(auto_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "827/827 [==============================] - 2s 2ms/step - loss: 0.8010 - accuracy: 0.7621\n",
            "[0.800967276096344, 0.7621225118637085]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCc3gtf3FpB5",
        "outputId": "90d8bab5-2d0d-4367-f314-88113ddd308a"
      },
      "source": [
        "model = auto_model.export_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 676)]             0         \n",
            "_________________________________________________________________\n",
            "cast_to_float32 (CastToFloat (None, 676)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                21664     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               16896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 36)                1188      \n",
            "_________________________________________________________________\n",
            "classification_head_2 (Softm (None, 36)                0         \n",
            "=================================================================\n",
            "Total params: 58,468\n",
            "Trainable params: 57,316\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}