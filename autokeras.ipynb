{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autokeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7dT7FaKCQq8VrjZptunfB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreasfloros/ARM-ML-Embedded/blob/main/autokeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RqHhQ2B2ZPh"
      },
      "source": [
        "## Prepare and preprocess data\n",
        "Referenced from audio_classifier pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RhgqzAxg4at",
        "outputId": "8f046c2f-63aa-4f81-ab76-90d3e75a41a2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO_nAYsFE33I"
      },
      "source": [
        "DATASET_ROOT_DIR = '/gdrive/MyDrive/audio_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "-xNjJ3_12C73",
        "outputId": "4b81d12b-9674-4207-c0d5-5770020b4da6"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "\n",
        "os.mkdir(DATASET_ROOT_DIR)\n",
        "url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "target_path = 'audio_data/dataset.tar.gz'\n",
        "\n",
        "response = requests.get(url, stream=True)\n",
        "if response.status_code == 200:\n",
        "    with open(target_path, 'wb') as f:\n",
        "        f.write(response.raw.read())\n",
        "\n",
        "tar = tarfile.open(target_path, \"r:gz\")\n",
        "tar.extractall(path='audio_data/')\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1d4ed88f9ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_ROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'audio_data/dataset.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/gdrive/MyDrive/audio_data/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCUii9Oe2GmC"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import math\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wr7TUFIiGuk"
      },
      "source": [
        "!mv /content/audio_data /gdrive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izb_V3B_2KSk"
      },
      "source": [
        "def make_track_correct_size(signal, expected_num_samples_per_track):\n",
        "\n",
        "    # print('Original track length: {}'.format(len(signal)))\n",
        "    # if track is shorter than expected, append it with zeros\n",
        "    if len(signal) < expected_num_samples_per_track:\n",
        "      num_zeros_to_pad = expected_num_samples_per_track - len(signal)\n",
        "      zeros = num_zeros_to_pad * [0.]\n",
        "      extended_signal = np.append(signal, zeros)\n",
        "      return extended_signal\n",
        "\n",
        "    # if track is longer than expected, truncate it\n",
        "    elif len(signal) > expected_num_samples_per_track:\n",
        "      return signal[:expected_num_samples_per_track]\n",
        "\n",
        "    # else return the original track \n",
        "    else:\n",
        "      return signal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSY5UKl82NTo"
      },
      "source": [
        "def audio_track_to_features(signal, processing_method, sample_rate, window_size, window_stride, num_mfcc):\n",
        "  \n",
        "  if processing_method == 'fft':\n",
        "    # perform Fast Fourier Transform (FFT)\n",
        "    fft = np.fft.fft(signal)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrum = np.abs(fft)\n",
        "\n",
        "    # the spectrum is symmetrical with respect to sample_rate / 2\n",
        "    # so take half of the spectrum and frequency arrays\n",
        "    # therefore len(half_spectrum) = sample_rate / 2\n",
        "    half_spectrum = spectrum[:int(len(spectrum)/2)]\n",
        "\n",
        "    # average every 16 samples to reduce size of array to 1 / 16 of its original size\n",
        "    # e.g. sample_rate = 16k, duration = 1.024s, reduce size from 8192 to 512 \n",
        "    averaged = np.mean(half_spectrum.reshape(-1, 16), axis=1)\n",
        "    return averaged\n",
        "\n",
        "\n",
        "  elif processing_method == 'stft':\n",
        "    # perform Short Time Fourier Transform (STFT)\n",
        "    stft = librosa.stft(signal = signal, \n",
        "                        n_fft = window_size, \n",
        "                        hop_length = window_stride)\n",
        "\n",
        "    # calculate abs values on complex numbers to get magnitude\n",
        "    spectrogram = np.abs(stft)\n",
        "\n",
        "    # transpose and return the spectrogram matrix\n",
        "    transposed_spectrogram = spectrogram.transpose()\n",
        "    return transposed_spectrogram.flatten()\n",
        "\n",
        "\n",
        "  else: # mfcc\n",
        "    # perform Mel-Frequency Cepstral Coefficients (MFCC)\n",
        "    mfcc = librosa.feature.mfcc(signal, \n",
        "                                sr = sample_rate, \n",
        "                                n_fft = window_size, \n",
        "                                n_mfcc = num_mfcc,\n",
        "                                hop_length = window_stride)\n",
        "    # transpose and return the mfcc matrix\n",
        "    transposed_mfcc = mfcc.T\n",
        "    return transposed_mfcc.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9y3B21e2Pkn"
      },
      "source": [
        "def preprocess_entire_dataset(dataset_path, json_path, processing_method, sample_rate, expected_duration, window_size, window_stride, num_mfcc):\n",
        "  # expected duration is in seconds\n",
        "  expected_num_samples_per_track = int(expected_duration * sample_rate)\n",
        "  \n",
        "  # dictionary to later be converted to final json file\n",
        "  data = {\n",
        "      'mapping' : [],\n",
        "      'features' : [],\n",
        "      'labels' : []\n",
        "  }\n",
        "\n",
        "  # we will iterate this for each of the visited sub-directorie in order to\n",
        "  # give a different label for each of them\n",
        "  visited_directory_index = 0\n",
        "\n",
        "  # iterate through all subfolders\n",
        "  for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
        "\n",
        "    # # ensure we are not at the dataset root directory\n",
        "    # # (os.walk provides this directory as well)\n",
        "    if dirpath is not DATASET_ROOT_DIR:\n",
        "    # if dirpath == 'audio_data/yes' or dirpath == 'audio_data/no':\n",
        "\n",
        "      # obtain word labels\n",
        "      dirpath_components = dirpath.split('/') # audio_data/left => ['audio_data', 'left']\n",
        "      word_label = dirpath_components[-1]\n",
        "      data['mapping'].append(word_label)\n",
        "      print('Processing {}'.format(word_label))\n",
        "\n",
        "      # access and process files for current word\n",
        "      for f in filenames:\n",
        "        \n",
        "        # load audio file\n",
        "        file_path = os.path.join(dirpath, f)\n",
        "        signal, sample_rate = librosa.load(file_path, sr=sample_rate)\n",
        "\n",
        "        # extend or cut signal to be equal to the expected size\n",
        "        signal_correct_size = make_track_correct_size(signal, expected_num_samples_per_track)\n",
        "\n",
        "        # obtain the features of the audio track using the function defined above\n",
        "        track_features = audio_track_to_features(signal = signal_correct_size, \n",
        "                                                 processing_method = processing_method,\n",
        "                                                 sample_rate = sample_rate, \n",
        "                                                 window_size = window_size, \n",
        "                                                 window_stride = window_stride, \n",
        "                                                 num_mfcc = 13)\n",
        "        \n",
        "        # append the audio track features to the features field of the dictionary\n",
        "        data['features'].append(track_features.tolist())\n",
        "\n",
        "        # append the current directory index as the label of this track\n",
        "        data['labels'].append(visited_directory_index)\n",
        "        # print('file_path: {}'.format(file_path))\n",
        "\n",
        "      # iterate the index before visiting the next directory\n",
        "      visited_directory_index = visited_directory_index + 1\n",
        "\n",
        "  print(data['mapping'])\n",
        "  print(set(data['labels']))\n",
        "  # create the json file from the dictionary\n",
        "  with open(json_path, 'w') as fp:\n",
        "    json.dump(data, fp, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ50Y9zJ2Rd1"
      },
      "source": [
        "rm -rf `find -type d -name .ipynb_checkpoints`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH9VWheB2Sy4",
        "outputId": "d9bea4c7-8022-4eaa-e28a-4e529af47110"
      },
      "source": [
        "!rm /gdrive/MyDrive/audio_data/_background_noise_/README.md"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/gdrive/MyDrive/audio_data/_background_noise_/README.md': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "hxWiWhl62UkB",
        "outputId": "cf492eb7-6569-4508-b3eb-08661e4498a8"
      },
      "source": [
        "JSON_PATH = DATASET_ROOT_DIR+'data.json'\n",
        "# DATASET_ROOT_DIR = '/gdrive/MyDrive/audio_data/'\n",
        "PROCESSING_METHOD = 'mfcc'\n",
        "SAMPLE_RATE = 16000\n",
        "EXPECTED_DURATION = 1.024            # in seconds\n",
        "WINDOW_SIZE_SAMPLES = 512            # in samples\n",
        "WINDOW_STRIDE_SAMPLES = 320          # in samples\n",
        "MFCC_COEFF_NUMBER = 13\n",
        "\n",
        "preprocess_entire_dataset(dataset_path = DATASET_ROOT_DIR, \n",
        "                   json_path = JSON_PATH, \n",
        "                   processing_method = PROCESSING_METHOD,\n",
        "                   sample_rate = SAMPLE_RATE, \n",
        "                   expected_duration = EXPECTED_DURATION, \n",
        "                   window_size = WINDOW_SIZE_SAMPLES, \n",
        "                   window_stride = WINDOW_STRIDE_SAMPLES, \n",
        "                   num_mfcc = MFCC_COEFF_NUMBER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing dog\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-60bccd70acf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                    \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWINDOW_SIZE_SAMPLES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                    \u001b[0mwindow_stride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWINDOW_STRIDE_SAMPLES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                    num_mfcc = MFCC_COEFF_NUMBER)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-9c0fed1e72e0>\u001b[0m in \u001b[0;36mpreprocess_entire_dataset\u001b[0;34m(dataset_path, json_path, processing_method, sample_rate, expected_duration, window_size, window_stride, num_mfcc)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# load audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# extend or cut signal to be equal to the expected size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfilesystemencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mfile_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_open_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmUMs8783yss"
      },
      "source": [
        "Import libraries for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2io5ne8u7MtT"
      },
      "source": [
        "!pip install autokeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "M7Bq2jmb2ZFq",
        "outputId": "0a23ef26-57d7-4f8f-e4db-8a734e357336"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Reshape, Activation, BatchNormalization, Conv2D, MaxPooling2D, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "import autokeras as ak\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-79c21814d029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkerastuner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autokeras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuO4KC753wtT"
      },
      "source": [
        "Load the data and prepare datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FVsHXdH3tw5"
      },
      "source": [
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    print('Loading dataset')\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    X = np.array(data['features'])\n",
        "    y = np.array(data['labels'])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
        "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
        "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
        "    :return X_train (ndarray): Input training set\n",
        "    :return X_validation (ndarray): Input validation set\n",
        "    :return X_test (ndarray): Input test set\n",
        "    :return y_train (ndarray): Target training set\n",
        "    :return y_validation (ndarray): Target validation set\n",
        "    :return y_test (ndarray): Target test set\n",
        "    \"\"\"\n",
        "\n",
        "    print('Splitting dataset into training, validation, and test splits')\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(JSON_PATH)\n",
        "\n",
        "    # if flatten == False:\n",
        "    #   Reshape(reshape_shape, input_shape=input_shape)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    # X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # add an axis to input sets for conv networks to fit conv2D shape specs\n",
        "    # currently not needed since we reshape everything anyway\n",
        "    # X_train = X_train[..., np.newaxis]\n",
        "    # X_validation = X_validation[..., np.newaxis]\n",
        "    # X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8tnRKJeAM56",
        "outputId": "85e33901-8dc5-4aa0-8e1c-42a56629717f"
      },
      "source": [
        "X_train, X_test, y_train, y_test = prepare_datasets(0.25, 0.2)\n",
        "print('Finished preparing training, validation, and test data')\n",
        "print('X_train.shape: {}'.format(X_train.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting dataset into training, validation, and test splits\n",
            "Loading dataset\n",
            "Finished preparing training, validation, and test data\n",
            "X_train.shape: (79376, 676)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZwhhJTYwfal",
        "outputId": "f0d56dd6-c7f5-445d-843b-3e07bfb0d874"
      },
      "source": [
        "NUM_OF_SAMPLES = SAMPLE_RATE * EXPECTED_DURATION\n",
        "NUM_OF_WINDOW_POSITIONS = math.ceil(NUM_OF_SAMPLES / WINDOW_STRIDE_SAMPLES)\n",
        "NUM_OF_STFT_FREQUENCIES = int(1 + WINDOW_SIZE_SAMPLES / 2)\n",
        "\n",
        "FFT_ARRAY_SIZE = int((NUM_OF_SAMPLES) / (2 * 16))  # 2 comes from half spectrum, 16 from averaging every 16 samples\n",
        "\n",
        "if PROCESSING_METHOD == 'fft':\n",
        "  RESHAPE_SHAPE = (1, FFT_ARRAY_SIZE, 1)\n",
        "elif PROCESSING_METHOD == 'stft':\n",
        "  RESHAPE_SHAPE = (NUM_OF_WINDOW_POSITIONS, NUM_OF_STFT_FREQUENCIES, 1)\n",
        "else: # mfcc\n",
        "  RESHAPE_SHAPE = (NUM_OF_WINDOW_POSITIONS, MFCC_COEFF_NUMBER, 1)\n",
        "\n",
        "print('RESHAPE_SHAPE: {}'.format(RESHAPE_SHAPE))\n",
        "INPUT_SHAPE = X_train.shape[1:]\n",
        "print('INPUT_SHAPE:{}'.format(INPUT_SHAPE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RESHAPE_SHAPE: (52, 13, 1)\n",
            "INPUT_SHAPE:(676,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FUDy8T84Qgu"
      },
      "source": [
        "Build AutoKera Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Xa-IeZ3fva"
      },
      "source": [
        "class SingleLayerReshapeBlock(ak.Block):\n",
        "  def __init__(self, input_shape, reshape_shape):\n",
        "    super().__init__()\n",
        "    self.input_shape = input_shape\n",
        "    self.reshape_shape = reshape_shape\n",
        "\n",
        "  def build(self, hp, inputs=None):\n",
        "      # Get the input_node from inputs.\n",
        "    input_node = tf.nest.flatten(inputs)[0]\n",
        "    layer = Reshape(self.reshape_shape, input_shape=self.input_shape)\n",
        "    output_node = layer(input_node)\n",
        "    return output_node\n",
        "\n",
        "  # def __call__(self, *args, **kwargs):\n",
        "  #   return self.build(*args, **kwargs)\n",
        "\n",
        "def build_conv_model():\n",
        "  hp = kt.HyperParameters()\n",
        "  filters = hp.Choice(\"filters\", values=[8,16,32])\n",
        "\n",
        "  input_node = ak.Input()\n",
        "  output_node = SingleLayerReshapeBlock(INPUT_SHAPE, RESHAPE_SHAPE)(input_node)\n",
        "  output_node = ak.ConvBlock(filters=filters, separable=True, max_pooling=True)(output_node)\n",
        "  output_node = ak.ClassificationHead()(output_node)\n",
        "  auto_model = ak.AutoModel(\n",
        "      inputs=input_node, outputs=output_node, objective=\"accuracy\", tuner=\"greedy\", overwrite=True, max_trials=10, max_model_size=50000\n",
        "  )\n",
        "  auto_model.fit(X_train, y_train, epochs=10)\n",
        "  return auto_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n656I6A62aP",
        "outputId": "0bb33935-f867-4f84-dde3-4846776ff5e6"
      },
      "source": [
        "auto_model = build_conv_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 03m 06s]\n",
            "accuracy: 0.6004408001899719\n",
            "\n",
            "Best accuracy So Far: 0.6157588362693787\n",
            "Total elapsed time: 00h 27m 28s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/10\n",
            "2481/2481 [==============================] - 18s 7ms/step - loss: 5.6634 - accuracy: 0.0836\n",
            "Epoch 2/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 2.5323 - accuracy: 0.3007\n",
            "Epoch 3/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 2.0694 - accuracy: 0.4320\n",
            "Epoch 4/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 1.8441 - accuracy: 0.4958\n",
            "Epoch 5/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 1.6940 - accuracy: 0.5355\n",
            "Epoch 6/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 1.5937 - accuracy: 0.5611\n",
            "Epoch 7/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 1.5231 - accuracy: 0.5800\n",
            "Epoch 8/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 1.4711 - accuracy: 0.5929\n",
            "Epoch 9/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 1.4286 - accuracy: 0.6024\n",
            "Epoch 10/10\n",
            "2481/2481 [==============================] - 17s 7ms/step - loss: 1.3933 - accuracy: 0.6112\n",
            "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T_T27FV3zDC",
        "outputId": "edc4009f-0413-482c-8eaa-c8f1a513d07e"
      },
      "source": [
        "print(auto_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "827/827 [==============================] - 3s 4ms/step - loss: 1.3950 - accuracy: 0.6114\n",
            "[1.395034670829773, 0.6113987565040588]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK2sfsWI31Po",
        "outputId": "1070401a-97dc-4c43-a750-f9e6c39339da"
      },
      "source": [
        "model = auto_model.export_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 676)]             0         \n",
            "_________________________________________________________________\n",
            "cast_to_float32 (CastToFloat (None, 676)               0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 52, 13, 1)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 48, 9, 8)          41        \n",
            "_________________________________________________________________\n",
            "separable_conv2d_1 (Separabl (None, 48, 9, 8)          272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 2, 8)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 192)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 36)                6948      \n",
            "_________________________________________________________________\n",
            "classification_head_1 (Softm (None, 36)                0         \n",
            "=================================================================\n",
            "Total params: 7,261\n",
            "Trainable params: 7,261\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm8I-Tlv_Dm1",
        "outputId": "0b45e05d-9d8e-49e7-debc-caac4bf2d42a"
      },
      "source": [
        "def build_dense_model():\n",
        "  hp = kt.HyperParameters()\n",
        "  num_units = hp.Choice(\"num_units\", values=[16,32,64,128,256])\n",
        "  input_node = ak.Input()\n",
        "  output_node = ak.DenseBlock(num_units=num_units)(input_node)\n",
        "  output_node = ak.ClassificationHead(num_classes=36, loss = \"categorical_crossentropy\")(output_node)\n",
        "  auto_model = ak.AutoModel(\n",
        "      inputs=input_node, outputs=output_node, objective=\"accuracy\", tuner=\"greedy\", overwrite=True, max_trials=10\n",
        "  )\n",
        "  auto_model.fit(X_train, y_train, epochs=10)\n",
        "  return auto_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 01m 28s]\n",
            "accuracy: 0.7838003635406494\n",
            "\n",
            "Best accuracy So Far: 0.7838003635406494\n",
            "Total elapsed time: 00h 11m 28s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/10\n",
            "2481/2481 [==============================] - 10s 3ms/step - loss: 3.1792 - accuracy: 0.1637\n",
            "Epoch 2/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.5075 - accuracy: 0.5591\n",
            "Epoch 3/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 1.2067 - accuracy: 0.6423\n",
            "Epoch 4/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 1.0683 - accuracy: 0.6821\n",
            "Epoch 5/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 0.9757 - accuracy: 0.7081\n",
            "Epoch 6/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.9055 - accuracy: 0.7289\n",
            "Epoch 7/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.8470 - accuracy: 0.7462\n",
            "Epoch 8/10\n",
            "2481/2481 [==============================] - 8s 3ms/step - loss: 0.7958 - accuracy: 0.7614\n",
            "Epoch 9/10\n",
            "2481/2481 [==============================] - 9s 3ms/step - loss: 0.7515 - accuracy: 0.7739\n",
            "Epoch 10/10\n",
            "2481/2481 [==============================] - 9s 4ms/step - loss: 0.7143 - accuracy: 0.7856\n",
            "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4qzykVlSoK-"
      },
      "source": [
        "auto_model = build_dense_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmjqELmxDdw7",
        "outputId": "1ec1b86a-04f8-4af4-a0f4-0cc6a68aa87e"
      },
      "source": [
        "print(auto_model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "827/827 [==============================] - 2s 2ms/step - loss: 0.8010 - accuracy: 0.7621\n",
            "[0.800967276096344, 0.7621225118637085]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCc3gtf3FpB5",
        "outputId": "90d8bab5-2d0d-4367-f314-88113ddd308a"
      },
      "source": [
        "model = auto_model.export_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 676)]             0         \n",
            "_________________________________________________________________\n",
            "cast_to_float32 (CastToFloat (None, 676)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                21664     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               16896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 36)                1188      \n",
            "_________________________________________________________________\n",
            "classification_head_2 (Softm (None, 36)                0         \n",
            "=================================================================\n",
            "Total params: 58,468\n",
            "Trainable params: 57,316\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}